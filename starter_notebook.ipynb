{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Electric Production Forecasting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Business Understanding\n",
    "## 1.1 Overview\n",
    "\n",
    "With their recent successful real estate investment in California, our client Homegates Property Group seeks to expand their listings in the state of New York, which happens to be the state with the most Fortune 500 company headquarters in the United States and the world. Our client believes the trends and contributing factors that they saw in California, especially the continued job growth will also positively impact the home values in New York. The team is looking for recommendations on top 5 zipcodes to invest in New York, and this analysis will also provide them with short-term vs. long-term investment decisions.\n",
    "\n",
    "## 1.2 Problem Statement\n",
    "\n",
    "The goal of this analysis is to identify the top 5 zipcodes for our client to invest in New York. The team is not quite familiar with the East Coast real estate market, and therefore, has asked to take risk factor into consideration. The results from this analysis will provide them with the forecast of next 10 year mean house values in the top 5 zipcodes as well as expected ROI in 1 year, 3 years, 5 years, and 10 years.\n",
    "\n",
    "## 1.3 Objective\n",
    "\n",
    " 1. To determine the expected ROI yield in 1 year, 3 years, 5 years, and 10 years.\n",
    " 2. To identify the top 5 zipcodes for our client to invest in New York based on the ROI yields.\n",
    " \n",
    "## 1.4 Metric of Success\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Understanding\n",
    "## 2.1 Data Overview\n",
    "\n",
    "## 2.2 Data Description\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Load the Data/Filtering for Chosen Zipcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(123)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "from matplotlib import rcParams\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data into a dataframe and preview\n",
    "df = pd.read_csv('data/electric_production.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general description of the data\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The data type for date column is object and this should be changed to datetime format.\n",
    "* The column name for IPG2211A2N column is not clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the rows and colums\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has 1006 rows and 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no dulpicates in our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preparation\n",
    "#### Validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the column name\n",
    "df.rename(columns={'IPG2211A2N':'energy_production(quads)', 'DATE':'date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change date column data type to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['date'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the date as index\n",
    "df.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data spans for a very long period of time i.e 1939 to 2022, we decided to slice the data and work with more recent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slice the data from the year 1990\n",
    "new_df = df['1990':]\n",
    "#preview the new data\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: EDA and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram and Density plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 15,8\n",
    "sns.distplot(new_df, hist=True, kde=True, \n",
    "             color = 'darkblue', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 4}\n",
    "             )\n",
    "plt.xlabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the plot above the data is somewhat normally distributed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series line plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.plot(figsize=(14,8))\n",
    "plt.ylabel('Energy production')\n",
    "plt.title('Energy production in USA over time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed a periodic and upward trend from 1990 to 2009 since the energy production increases over time and then after 2009 we have seasonal variation in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2015 =  df['2015':]\n",
    "df_2015.plot(figsize=(14,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping and visualizing time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use pandas grouper to group values using annual frequency\n",
    "# year_groups = new_df['2010':].groupby(pd.Grouper(freq ='A'))\n",
    "# len(year_groups['date'])\n",
    "\n",
    "#year_groups.plot(figsize=(14, 8),subplots=True, legend=True)\n",
    "\n",
    "# #Create a new DataFrame and store yearly values in column\n",
    "# s \n",
    "# us_annual = pd.DataFrame()\n",
    "\n",
    "# for yr, group in year_groups:\n",
    "#     us_annual[yr.year] = group.values.ravel()\n",
    "    \n",
    "# #Plot the yearly groups as subplots\n",
    "# us_annual.plot(figsize = (13,8), subplots=True, legend=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Trends\n",
    "We can see that that there is an overall increasing trend in the data along with some seasonal variations. However, it might not always be possible to make such visual inferences. Let's reconfirm this here using both rolling statistics and the Dickey-Fuller test\n",
    "\n",
    "### Rolling Statistics and The Dickey-Fuller Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stationarity_test(df, window = 12):\n",
    "    \n",
    "    roll_mean = df.rolling(window).mean()\n",
    "    roll_std = df.rolling(window).std()\n",
    "    \n",
    "    plt.figure(figsize = (10,6))\n",
    "    original = plt.plot(df, color='cornflowerblue',\n",
    "                    label='Original')\n",
    "    mean = plt.plot(roll_mean, color='firebrick',\n",
    "                    label='Rolling Mean')\n",
    "    std = plt.plot(roll_std, color='limegreen',\n",
    "                   label='Rolling Std')\n",
    "    plt.legend(loc = 0)\n",
    "    plt.title('Rolling Statistics', size = 14)\n",
    "    plt.show(block=False)\n",
    "\n",
    "    print(\"The Dickey-Fuller Test \\n\")\n",
    "    \n",
    "    adf = adfuller(df, autolag='AIC')\n",
    "    \n",
    "    print('ADF Statistic: {}'.format(round(adf[0],3)))\n",
    "    print('p-value: {}'.format(round(adf[1],3)))\n",
    "    print(\"-------------------------------------------\")  \n",
    "    print('Critical Values:')\n",
    "    \n",
    "    for key, ts in adf[4].items():\n",
    "         print('{}: {}'.format(key, round(ts,3)))\n",
    "    print(\"-------------------------------------------\")\n",
    "    \n",
    "    if adf[0] > adf[4][\"5%\"]:\n",
    "        print(\"ADF > Critical Values\")\n",
    "        print (\"Failed to reject null hypothesis, time series is non-stationary.\")\n",
    "    else:\n",
    "        print(\"ADF < Critical Values\")\n",
    "        print (\"Reject null hypothesis, time series is stationary.\")\n",
    "        \n",
    "stationarity_test(new_df, window = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the variation in standard deviation is small, the mean is increasing with time and thus, this is not a stationary series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null-hypothesis for the test is that the time series is not stationary. The p-value is obtained is greater than significance level of 0.05 and the ADF statistic is higher than any of the critical values.\n",
    "\n",
    "Clearly, there is no reason to reject the null hypothesis. So, the time series is in fact non-stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing trends\n",
    "## Overview\n",
    "Time series models work on the assumption that the time series are stationary and in this case our time series is non-stationary. We need to remove the trends to convert our time series to stationary. We will employ the following methods:\n",
    "\n",
    "* Log transformation\n",
    "* Subtracting the rolling mean\n",
    "* Decompositon\n",
    "\n",
    "### `Log transformation`\n",
    "\n",
    "One way to enforce stationarity can be a simple log transformation to make the time series more \"uniform\" over time.\n",
    "\n",
    "The advantage of taking a log transformation is that higher values are penalized more than lower values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = np.log(new_df['energy_production(quads)'])\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.plot(log_df, color='blue')\n",
    "plt.xlabel('years', fontsize=14)\n",
    "plt.ylabel('log(energy production)', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for ststionarity\n",
    "stationarity_test(log_df, window = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Subtracting the rolling mean`\n",
    "Since the log transformed time series data above is non stationary, we will subtract the rolling mean from the log transformed time series data in an attempt to make it stationary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating and visualizing rolling mean and log transformed data\n",
    "roll_mean = log_df.rolling(window=4).mean()\n",
    "\n",
    "fig = plt.figure(figsize=(11,7))\n",
    "plt.plot(log_df, color='blue',label='log transformed data')\n",
    "plt.plot(roll_mean, color='red', label='Rolling Mean')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Rolling Mean and log transformed data')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtracting rolling mean from log transformed data\n",
    "logdf_minus_roll_mean = log_df - roll_mean\n",
    "logdf_minus_roll_mean.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the missing values from time series calculated above\n",
    "logdf_minus_roll_mean.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing rolling mean subtracted from log transformed data\n",
    "fig = plt.figure(figsize=(11,7))\n",
    "plt.plot(logdf_minus_roll_mean, color='blue',label='log_df - rolling mean')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Energy production while the rolling mean is subtracted')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for stationarity\n",
    "stationarity_test(logdf_minus_roll_mean, window = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Decompositon`\n",
    "Since our data is already stationary ,we decided to do decomposition in order to have a clear picture of trend,seasonal and residuals.\n",
    "\n",
    "Time series decomposition is a mathematical procedure that transforms a time series into multiple different time series. The original time series is often split into three component series:\n",
    "* Seasonal: Patterns that repeat within a fixed period.\n",
    "* Trend: The underlying trend of the metrics.\n",
    "* Random: Also called \"noise\", \"irregular\", or \"remainder\", this is the residual of the original time series after the seasonal and trend series are removed\n",
    "\n",
    "To achieve successful decomposition, it is important to choose between the additive and multiplicative models, which requires analyzing the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and apply seasonal_decompose()\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decomposition = seasonal_decompose(log_df)\n",
    "\n",
    "# Gather the trend, seasonality, and residuals \n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "# Plot gathered statistics\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.subplot(411)\n",
    "plt.plot(log_df, label='log transformed data', color='cornflowerblue')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label='Trend', color='cornflowerblue')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal,label='Seasonality', color='cornflowerblue')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label='Residuals', color='cornflowerblue')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the trend and seasonality are separated from data and we can model the residuals.\n",
    "The plot above clearly shows the upward trend of our data, along with its yearly seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values from residuals\n",
    "logdf_decompose = residual\n",
    "logdf_decompose.dropna(inplace=True)\n",
    "# Check stationarity\n",
    "stationarity_test(logdf_decompose, window = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dickey-Fuller test statistic is significant and very well below a (strict) 0.01 mark. So it seems reasonable to assume this time series is stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation and  Partial Autocorrelation \n",
    "    \n",
    "Autocorrelation analysis is an important step in the Exploratory Data Analysis of time series forecasting. The autocorrelation analysis helps detect patterns and check for randomness. It’s especially important when you intend to use an autoregressive–moving-average (ARMA) model for forecasting because it helps to determine its parameters. The analysis involves looking at the Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots.\n",
    "\n",
    "To determine the order of the model, you check the number of  lollipops above or below the confidence interval before the next lollipop enters the blue area\n",
    "\n",
    "The ACF plot will provide answers to the following questions:\n",
    "\n",
    "* Is the observed time series white noise/random?\n",
    "* Is an observation related to an adjacent observation, an observation twice-removed, and so on?\n",
    "* Can the observed time series be modeled with an MA model? If yes, what is the order?\n",
    "\n",
    "The PACF plot will provide answers to the following question:\n",
    "\n",
    "* Can the observed time series be modeled with an AR model? If yes, what is the order?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pylab import rcParams\n",
    "\n",
    "rcParams['figure.figsize'] = 14, 5\n",
    "\n",
    "plot_acf(logdf_decompose, lags = 24)\n",
    "\n",
    "plot_pacf(logdf_decompose, lags=24);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auto Regressive(AR) observation \n",
    "* There are several autocorrelations that are significantly non-zero. Therefore, the time series is non-random.\n",
    "\n",
    "* High degree of autocorrelation between adjacent (lag = 1) and near-adjacent (lag = 5) observations in PACF plot\n",
    "\n",
    "* Geometric decay in ACF plot\n",
    "\n",
    "#### Moving Average (MA)\n",
    "* There are several autocorrelations that are significantly non-zero. Therefore, the time series is non-random.\n",
    "\n",
    "* High degree of autocorrelation between adjacent (lag = 1) in ACF plot\n",
    "\n",
    "* Geometric decay in PACF plot\n",
    "\n",
    "\n",
    "Based on the observations above, AR model will have an order of 5 and the MA model an order of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "We are going to use \n",
    "\n",
    "#### Splitting data into train and test for modelling\n",
    "\n",
    "We have used the first 25 years as the training dataset and the last 7 years as the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ar_values = logdf_decompose.values\n",
    "# train = ar_values[1:len(ar_values)-82] \n",
    "# test = ar_values[len(ar_values)-83:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = logdf_decompose[:-82]\n",
    "test = logdf_decompose[-82:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Auto regressive model`\n",
    "An autoregressive (AR) model is when a value from a time series is regressed on previous values from the same time series.\n",
    "\n",
    "The order of the model is in the form (p, d, q):\n",
    "* p refers to the order of AR which will be `5` in this case.\n",
    "* d refers to the order of I which will be `0` in this case.\n",
    "* q refers to the order of MA which will be `0` in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate AR model with an order of 5\n",
    "ar_model = ARIMA(train, order=(5,0,0))\n",
    "\n",
    "# fitting the model\n",
    "ar_fitted = ar_model.fit()\n",
    "\n",
    "# print out summary information of the fit\n",
    "print(ar_fitted.summary())\n",
    "\n",
    "print('\\nEstimate for the constant and for theta:\\n')\n",
    "\n",
    "# Print out the estimate for the constant and for theta\n",
    "\n",
    "print(ar_fitted.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_predictions = ar_fitted.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\n",
    "ar_score = mean_squared_error(test, ar_predictions)\n",
    "print('AR MSE: {}'.format(round(ar_score,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the true values of the test and the predicted values\n",
    "plt.figure(figsize = (10,6))\n",
    "plt.plot(test, label = \"true values\", color = \"cornflowerblue\")\n",
    "plt.plot(ar_predictions,label = \"forecasts\", color='darkorange')\n",
    "plt.title(\"AR Model\", size = 14)\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Moving average model`\n",
    "\n",
    "The Moving Average model can be described as the weighted sum of today's and yesterday's noise.\n",
    "\n",
    "The order of the model is in the form (p, d, q):\n",
    "* p refers to the order of AR which will be `0` in this case.\n",
    "* d refers to the order of I which will be `0` in this case.\n",
    "* q refers to the order of MA which will be `1` in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate MA model with an order of 1\n",
    "ma_model = ARIMA(train, order=(0,0,1))\n",
    "\n",
    "# fitting the model\n",
    "ma_fitted = ma_model.fit()\n",
    "\n",
    "# print out summary information of the fit\n",
    "print(ma_fitted.summary())\n",
    "\n",
    "print('\\nEstimate for the constant and for theta:\\n')\n",
    "\n",
    "# Print out the estimate for the constant and for theta\n",
    "\n",
    "print(ma_fitted.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_predictions = ma_fitted.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\n",
    "ma_score = mean_squared_error(test, ma_predictions)\n",
    "print('MA MSE: {}'.format(round(ma_score,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "plt.plot(test, label = \"true values\", color = \"cornflowerblue\")\n",
    "plt.plot(ma_predictions,label = \"forecasts\", color='darkorange')\n",
    "plt.title(\"MA Model\", size = 14)\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARIMA model\n",
    "\n",
    "The order of the model is in the form (p, d, q):\n",
    "* p refers to the order of AR which will be `5` in this case.\n",
    "* d refers to the order of I which will be `0` in this case.\n",
    "* q refers to the order of MA which will be `1` in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_model = ARIMA(train, order=(5,0,1))\n",
    "arima_fitted = arima_model.fit()\n",
    "\n",
    "arima_predictions = arima_fitted.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\n",
    "arima_score = mean_squared_error(test, arima_predictions)\n",
    "print('AR MSE: {}'.format(round(arima_score,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "plt.plot(test, label = \"true values\", color = \"cornflowerblue\")\n",
    "plt.plot(arima_predictions,label = \"forecasts\", color='darkorange')\n",
    "plt.title(\"ARIMA Model\", size = 14)\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = new_df.copy()\n",
    "ts.reset_index(inplace=True)\n",
    "ts.rename(columns={'date':'ds','energy_production(quads)':'y'},inplace =True)\n",
    "ts.set_index('ds',inplace=True)\n",
    "ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fbprophet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfbprophet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Prophet\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# instantiate the model and fit the timeseries\u001b[39;00m\n\u001b[0;32m      4\u001b[0m prophet \u001b[38;5;241m=\u001b[39m Prophet()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fbprophet'"
     ]
    }
   ],
   "source": [
    "from fbprophet import Prophet\n",
    "\n",
    "# instantiate the model and fit the timeseries\n",
    "prophet = Prophet()\n",
    "prophet.fit(ts)\n",
    "\n",
    "# create a future data frame \n",
    "future = prophet.make_future_dataframe(periods=24)\n",
    "forecast = prophet.predict(future)\n",
    "\n",
    "# display the most critical output columns from the forecast\n",
    "forecast[['ds','yhat','yhat_lower','yhat_upper']].head()\n",
    "\n",
    "# plot\n",
    "fig = prophet.plot(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "54dff4baf703f1537d376e0d6d6103c72b08fdf7559129a1afdd27c2d3217b5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
